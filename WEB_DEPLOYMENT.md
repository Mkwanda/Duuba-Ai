# Web Deployment Guide for Duuba-AI

## Overview
This guide covers deploying Duuba-AI to various cloud platforms. The app has been configured to work with cloud environments using relative paths and environment variables.

## Key Changes Made for Web Deployment

### 1. **Simplified File Paths**
   - Changed from `Cocoa/Tensorflow/workspace/models/...` to simple relative paths
   - Model path: `my_ssd_mobnetpod/`
   - Annotations: `annotations/`
   - This allows the app to work in any deployment environment

### 2. **Environment Variables**
   - Set `MODEL_URL` environment variable to download model automatically
   - Example: `export MODEL_URL="https://drive.google.com/file/d/YOUR_FILE_ID/view"`

### 3. **Cross-Platform Compatibility**
   - Replaced OS-specific mkdir commands with `os.makedirs()`
   - Works on Linux (Heroku, AWS, etc.) and Windows

## Deployment Options

### Option 1: Streamlit Cloud (Easiest)

1. **Push to GitHub**
   ```bash
   git init
   git add .
   git commit -m "Prepare for web deployment"
   git push origin main
   ```

2. **Deploy on Streamlit Cloud**
   - Go to [share.streamlit.io](https://share.streamlit.io)
   - Connect your GitHub repository
   - Add secrets in Streamlit Cloud dashboard:
     ```
     MODEL_URL = "https://drive.google.com/file/d/1Biw-K0DlbOAxGVEm4wy2LLGdDSuZl1Wg/view"
     ```
   - Click Deploy

3. **Model Files**
   - Either: Include model files in repo (if < 100MB after Git LFS)
   - Or: Set MODEL_URL to download on first run

### Option 2: Heroku

1. **Prerequisites**
   ```bash
   heroku login
   ```

2. **Create Heroku App**
   ```bash
   heroku create duuba-ai-cocoa-detector
   ```

3. **Set Environment Variables**
   ```bash
   heroku config:set MODEL_URL="https://drive.google.com/file/d/1Biw-K0DlbOAxGVEm4wy2LLGdDSuZl1Wg/view"
   ```

4. **Deploy**
   ```bash
   git push heroku main
   ```

5. **Files Required**
   - `Procfile` ✅ (already configured)
   - `requirements.txt` ✅
   - `runtime.txt` ✅

### Option 3: Docker (AWS, Azure, GCP)

1. **Build Docker Image**
   ```bash
   docker build -t duuba-ai .
   ```

2. **Run Locally**
   ```bash
   docker run -p 8501:8501 \
     -e MODEL_URL="https://drive.google.com/file/d/YOUR_ID/view" \
     duuba-ai
   ```

3. **Push to Container Registry**
   ```bash
   # AWS ECR
   docker tag duuba-ai:latest <account-id>.dkr.ecr.region.amazonaws.com/duuba-ai:latest
   docker push <account-id>.dkr.ecr.region.amazonaws.com/duuba-ai:latest
   
   # Deploy to AWS ECS, Azure Container Instances, or Google Cloud Run
   ```

### Option 4: Local Web Server

1. **Setup**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\\Scripts\\activate
   pip install -r requirements.txt
   ```

2. **Configure Environment**
   ```bash
   cp .env.example .env
   # Edit .env file with your MODEL_URL
   ```

3. **Run**
   ```bash
   streamlit run Duuba-AI.py
   ```

## Model Storage Options

### Google Drive (Current)
- **Pros**: Free, easy to share
- **Cons**: Slow downloads, rate limits
- **Setup**: Share file publicly, use link in MODEL_URL

### AWS S3
```bash
# Upload model
aws s3 cp my_ssd_mobnetpod/ s3://your-bucket/models/my_ssd_mobnetpod/ --recursive

# Set MODEL_URL
export MODEL_URL="https://your-bucket.s3.amazonaws.com/models/my_ssd_mobnetpod.tar.gz"
```

### Azure Blob Storage
```bash
# Upload
az storage blob upload-batch -d models -s my_ssd_mobnetpod/

# Get URL and set MODEL_URL
```

### GitHub Releases (< 2GB)
1. Create release on GitHub
2. Attach model.tar.gz as asset
3. Use direct download URL

## Required Model Files

Ensure your model directory contains:
```
my_ssd_mobnetpod/
├── checkpoint
├── ckpt-3.data-00000-of-00001
├── ckpt-3.index
└── pipeline.config
```

Additional files:
```
annotations/
└── label_map.pbtxt  # Auto-generated by app
```

## Environment Variables

| Variable | Required | Description | Example |
|----------|----------|-------------|---------|
| `MODEL_URL` | Yes* | URL to download model | Google Drive/S3 link |
| `PORT` | No | Server port (auto-detected on most platforms) | 8501 |

*Not required if model files are already in the deployment

## Troubleshooting

### "Model not found" error
1. Check MODEL_URL is set correctly
2. Verify model files are in `my_ssd_mobnetpod/` directory
3. Check network connectivity to download source

### Memory issues
- Use smaller batch size in pipeline.config
- Deploy on instance with >= 2GB RAM
- Consider TFLite model for edge deployment

### Slow loading
- Pre-download model during build (add to Dockerfile)
- Use cloud storage in same region as deployment
- Enable caching with `@st.cache_resource`

## Performance Optimization

1. **Pre-download Model**
   Add to Dockerfile:
   ```dockerfile
   RUN python download_model.py --url $MODEL_URL
   ```

2. **Use CDN for Model**
   - CloudFlare, AWS CloudFront
   - Reduces download time significantly

3. **Optimize Image Size**
   - Current: Resizes to 400x400 for display
   - Model input: Configurable in pipeline

## Security Notes

1. **API Keys**: Never commit to repository
2. **Model Access**: Use signed URLs for private models
3. **CORS**: Configured in .streamlit/config.toml
4. **Rate Limiting**: Consider adding for public deployments

## Monitoring

Add logging for production:
```python
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
```

## Cost Estimates

| Platform | Free Tier | Paid |
|----------|-----------|------|
| Streamlit Cloud | Yes (1 app) | $25/mo unlimited |
| Heroku | Limited hours | $7-25/mo |
| AWS ECS | 750 hrs/mo | $10-50/mo |
| Google Cloud Run | 2M requests | Pay per use |

## Support

For issues, check:
1. Error logs in platform dashboard
2. Model compatibility (TF 2.x)
3. Dependencies in requirements.txt

---

**Ready to deploy!** Choose your platform and follow the steps above.
